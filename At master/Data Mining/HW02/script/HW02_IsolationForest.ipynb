{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de703e0",
   "metadata": {},
   "source": [
    "## Isolation Forest（孤立森林）\n",
    "\n",
    "是一個非監督式的異常檢測演算法，其核心原理是不斷選取特徵去區分子群後，透過判別每一個資料點的深度，決定是否為異常值。\n",
    "\n",
    "Isolation Forest與一般需要計算密度、距離的演算法不同，透過特徵切分，如果是異常資料點，其特徵會與大多數正常資料不同，因此很容易在淺層被區分出來，因此透過計算樣本在每一棵樹的深度，就可以去區分樣本是否為異常值了！\n",
    "（通常計算效率高也可以處理大量資料）\n",
    "\n",
    "如果特徵選取剛好沒有鑑別力，造成深度很淺，造成誤以為是異常值怎麼辦？其實這個問題，可以透過多棵樹去避開這個問題，因為每一棵樹都會隨機選擇特徵以及threshold，多棵樹去綜合計算類似平均深度，就不容易誤判了！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e40d78",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb96d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import joblib  # Import joblib for saving the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915fb742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the datasets:\n",
      "    training (rows, cols) = (2940, 16)\n",
      "    validate (rows, cols) = (1260, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 設置 logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "# 2. 數據預處理\n",
    "input_path = '../dataSets'\n",
    "train_data = pd.read_csv(os.path.join(input_path, \"training.csv\"))\n",
    "\n",
    "# 分割出驗證集\n",
    "train_data = train_data.drop(columns=[\"lettr\"])\n",
    "\n",
    "VALIDATE_SIZE = 0.3\n",
    "RANDOM_SEED = 42\n",
    "X_train, X_valid = train_test_split(train_data, test_size=VALIDATE_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"\"\"Shape of the datasets:\n",
    "    training (rows, cols) = {X_train.shape}\n",
    "    validate (rows, cols) = {X_valid.shape}\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "# configure our pipeline\n",
    "pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                     ('scaler', MinMaxScaler())])\n",
    "\n",
    "pipeline.fit(X_train)\n",
    "\n",
    "x_train = pipeline.transform(X_train)\n",
    "x_valid = pipeline.transform(X_valid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f04d952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000000 seconds\n",
      "INFO:hyperopt.tpe:TPE using 0 trials\n",
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000536 seconds\n",
      "INFO:hyperopt.tpe:TPE using 1/1 trials with best loss 0.052843\n",
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000000 seconds\n",
      "INFO:hyperopt.tpe:TPE using 2/2 trials with best loss 0.049191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'contamination': 0.07826188124409683, 'max_samples': 0.6945413344448359, 'n_estimators': 1000}\n",
      "Best loss: 0.05284319155483954\n",
      "Best hyperparameters:  {'contamination': 0.09540391905903309, 'max_samples': 0.7895819578647452, 'n_estimators': 100}\n",
      "Best loss: 0.04919099034564791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.001000 seconds\n",
      "INFO:hyperopt.tpe:TPE using 3/3 trials with best loss 0.049191\n",
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000000 seconds\n",
      "INFO:hyperopt.tpe:TPE using 4/4 trials with best loss 0.049191\n",
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.001000 seconds\n",
      "INFO:hyperopt.tpe:TPE using 5/5 trials with best loss 0.049191\n",
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000000 seconds\n",
      "INFO:hyperopt.tpe:TPE using 6/6 trials with best loss 0.049191\n",
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.001001 seconds\n",
      "INFO:hyperopt.tpe:TPE using 7/7 trials with best loss 0.048760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'contamination': 0.09765716209726158, 'max_samples': 0.9008788036187498, 'n_estimators': 200}\n",
      "Best loss: 0.04875991769112048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.000000 seconds\n",
      "INFO:hyperopt.tpe:TPE using 8/8 trials with best loss 0.048760\n",
      "INFO:hyperopt.tpe:build_posterior_wrapper took 0.001002 seconds\n",
      "INFO:hyperopt.tpe:TPE using 9/9 trials with best loss 0.048437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'contamination': 0.09492990215205609, 'max_samples': 0.8157492680691261, 'n_estimators': 3000}\n",
      "Best loss: 0.04843745293005307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Best hyperparameters: {'contamination': 0.09492990215205609, 'max_samples': 0.8157492680691261, 'n_estimators': 3000}\n",
      "INFO:root:Best loss: 0.048437\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def calculate_isolation_loss(data, model):\n",
    "#     # 對每個樣本給定一個異常分數\n",
    "#     anomaly_scores = model.decision_function(data) \n",
    "#     isolation_errors = np.mean(np.power(anomaly_scores, 2)) # mse\n",
    "#     return isolation_errors\n",
    "\n",
    "\n",
    "\n",
    "# 目標函數\n",
    "def objective(params):\n",
    "    # 記錄開始時間\n",
    "    timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "    \n",
    "    # 建立 Isolation Forest 模型\n",
    "    model = IsolationForest(n_estimators=int(params['n_estimators']), \n",
    "                            # max_samples=params['max_samples'], \n",
    "                            max_samples='auto',\n",
    "                            contamination=params['contamination'], \n",
    "                            random_state=42)\n",
    "    \n",
    "    model.fit(x_train)\n",
    "    \n",
    "    # 計算異常分數\n",
    "    anomaly_scores = model.decision_function(x_valid) \n",
    "\n",
    "    # 計算異常分數平均誤差\n",
    "    val_loss = np.mean(anomaly_scores)\n",
    "    \n",
    "    # 保存當前最佳超參數和權重\n",
    "    if val_loss < objective.best_loss:  # 只在損失改進時儲存最佳權重和參數\n",
    "        objective.best_loss = val_loss\n",
    "        objective.best_params = params\n",
    "        \n",
    "        # 使用 joblib 儲存模型\n",
    "        model_dir = 'best_forest'\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        model_path = os.path.join(model_dir,'best_model_weights.pkl')\n",
    "        joblib.dump(model, model_path)  # 使用 joblib 保存模型\n",
    "        \n",
    "\n",
    "        # 顯示最佳超參數\n",
    "        print(\"Best hyperparameters: \", params)\n",
    "        print(f\"Best loss: {val_loss}\")\n",
    "      \n",
    "    return {'loss': val_loss, 'status': STATUS_OK, 'params': params}\n",
    "\n",
    "\n",
    "# 初始化\n",
    "objective.best_loss = float('inf')\n",
    "\n",
    "# 超參數搜尋空間\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 1000, 2000, 3000]),  # 樹的數量\n",
    "    'max_samples': hp.uniform('max_samples', 0.5, 1.0),             # 隨機樣本數量\n",
    "    'contamination': hp.uniform('contamination', 0.01, 0.1)         # 異常樣本的比例\n",
    "}\n",
    "\n",
    "# # 超參數搜尋空間\n",
    "# space = {\n",
    "#     'n_estimators': hp.choice('n_estimators', [100]),               # 樹的數量\n",
    "#     'max_samples': hp.uniform('max_samples', 0.5, 1.0),             # 隨機樣本數量\n",
    "#     'contamination': hp.uniform('contamination', 0.01, 0.1)         # 異常樣本的比例\n",
    "# }\n",
    "\n",
    "# 使用 Hyperopt 進行超參數搜尋\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, \n",
    "            space=space, \n",
    "            algo=tpe.suggest, \n",
    "            max_evals=10, \n",
    "            trials=trials, \n",
    "            verbose=0, \n",
    "            show_progressbar=True\n",
    "        )\n",
    "\n",
    "\n",
    "# 輸出最佳結果\n",
    "best_params = trials.best_trial['result']['params']\n",
    "best_loss = trials.best_trial['result']['loss']\n",
    "logger.info(\"Best hyperparameters: %s\", best_params) \n",
    "logger.info(\"Best loss: %f\", best_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e866645c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_X.csv\n",
      "test_loss: 0.03097978191847972\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "test_data = pd.read_csv(os.path.join(input_path, \"test_X.csv\"))\n",
    "x_test = pipeline.transform(test_data)\n",
    "\n",
    "# 載入最佳結果進行預測\n",
    "best_model_path = 'best_forest/best_model_weights.pkl'  # 儲存的最佳結果路徑\n",
    "best_model = joblib.load(best_model_path)  # 使用 joblib 載入模型\n",
    "\n",
    "# 計算異常分數\n",
    "anomaly_scores = best_model.decision_function(x_test) \n",
    "\n",
    "# 計算異常分數平均誤差\n",
    "test_loss = np.mean(anomaly_scores)\n",
    "print(f\"test_X.csv\")\n",
    "print(f\"test_loss: {test_loss}\")\n",
    "\n",
    "# 儲存結果\n",
    "results = pd.DataFrame({'id': test_data.index, 'outliers': anomaly_scores})\n",
    "results.to_csv(\"best_forest/anomaly_detection_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67dcda6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# # 讀取 CSV 檔案\n",
    "# input_path = 'best_forest/anomaly_detection_results_val.csv'  # 根據實際情況修改此路徑\n",
    "# result = pd.read_csv(input_path)\n",
    "# result_cout = len(result)\n",
    "# print(f\"Total data count for letters: {result_cout}\")\n",
    "\n",
    "\n",
    "# # 目標字母ID\n",
    "# target_ids = ['B', 'E', 'K', 'N', 'X', 'Z']\n",
    "\n",
    "# # 1. 找出 id 屬於 ['B', 'E', 'K', 'N', 'X', 'Z'] 且 outliers = -1 的情況\n",
    "# outliers_negative = result[(result['id'].isin(target_ids)) & (result['outliers'] == -1)]\n",
    "\n",
    "# # 2. 找出 id 不是 ['B', 'E', 'K', 'N', 'X', 'Z'] 且 outliers = 1 的情況\n",
    "# outliers_positive = result[~result['id'].isin(target_ids) & (result['outliers'] == 1)]\n",
    "\n",
    "# # 3. 統計符合上述條件的數量\n",
    "# total_outliers = len(outliers_negative) + len(outliers_positive)\n",
    "\n",
    "# # 顯示結果\n",
    "# print(f\"Count of outliers where id in ['B', 'E', 'K', 'N', 'X', 'Z'] and outliers = -1: {len(outliers_negative)}\")\n",
    "# print(f\"Count of outliers where id not in ['B', 'E', 'K', 'N', 'X', 'Z'] and outliers = 1: {len(outliers_positive)}\")\n",
    "# print(f\"Total count: {total_outliers}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
